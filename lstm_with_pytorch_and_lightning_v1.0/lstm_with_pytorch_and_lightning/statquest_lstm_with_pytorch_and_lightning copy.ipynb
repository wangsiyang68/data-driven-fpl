{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34006ee1-51e5-4a48-82de-021a71e1201a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# StatQuest: Long Short-Term Memory (LSTM) with PyTorch + Lightning!!!\n",
    "## Sponsored by...\n",
    "[<img src=\"./images/Brandmark_FullColor_Black.png\" alt=\"Lightning\" style=\"width: 400px;\">](https://lightning.ai/)\n",
    "\n",
    "Copyright 2022, Joshua Starmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3669a0-284c-48f7-9c73-19812e2a8fac",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6b9647-24a8-4c13-9665-a036d9a8e121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # torch will allow us to create tensors.\n",
    "import torch.nn as nn # torch.nn allows us to create a neural network.\n",
    "import torch.nn.functional as F # nn.functional give us access to the activation and loss functions.\n",
    "from torch.optim import Adam # optim contains many optimizers. This time we're using Adam\n",
    "\n",
    "import lightning as L # lightning has tons of cool tools that make neural networks easier\n",
    "from torch.utils.data import TensorDataset, DataLoader # these are needed for the training data\n",
    "\n",
    "## Set the seed so that, hopefully, everyone will get the same results as me.\n",
    "from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1076357-c957-44ed-947b-8267b4f4c3a9",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c4aed1-1e0c-4895-b96f-9dcfca679dd0",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "# Build a Long Short-Term Memory unit by hand using PyTorch + Lightning\n",
    "\n",
    "Just like we have done in previous tutorials, building a neural network, and a Long Short-Term Memory (LSTM) unit is a type of neural network, means we need to create a new class. To make it easy to train the LSTM, this class will inherit from `LightningModule` and we'll create the following methods:\n",
    "- `__init__()` to initialize the Weights and Biases and keep track of a few other house keeping things.\n",
    "- `lstm_unit()` to do the LSTM math. For example, to calculate the percentage of the long-term memory to remember.\n",
    "- `forward()` to make a forward pass through the unrolled LSTM. In other words `forward()` calls `lstm_unit()` for each data point.\n",
    "- `configure_optimizers()` to configure the opimimizer. In the past, we have use `SGD` (Stochastic Gradient Descent), however, in this tutorial we'll change things up and use `Adam`, another popular algorithm for optimizing the Weights and Biases.\n",
    "- `training_step()` to pass the training data to `forward()`, calculate the loss and to keep track of the loss values in a log file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f74907-ac4c-4dfd-aa53-78eaa1d6cbbb",
   "metadata": {},
   "source": [
    "Once we have created the class that defines an LSTM, we can use it to create a model and print out the randomly initialized Weights and Biases. Then, just for fun, we'll see what those random Weights and Biases predict for **Company A** and **Company B**. If they are good predictions, then we're done! However, the chances of getting good predictions from random values is very small. :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1116de-c45d-4d82-a727-73a5806fa18f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784e2424-649a-44ce-8a34-c98b8fced609",
   "metadata": {},
   "source": [
    "Since we are using **Lightning** training, training the LSTM we created by hand is pretty easy. All we have to do is create the training data and put it into a `DataLoader`..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96acff29-6f45-4840-b7ff-ce7e15a0cfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create the training data for the neural network.\n",
    "inputs = torch.tensor([[0., 0.5, 0.25, 1.], [1., 0.5, 0.25, 1.]])\n",
    "labels = torch.tensor([0., 1.])\n",
    "\n",
    "dataset = TensorDataset(inputs, labels) \n",
    "dataloader = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc4dc9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.size())\n",
    "print(labels.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff877386-0b56-4c60-bcd1-bfa123b9fe7f",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be30faf-c262-448a-b60d-8455ff9af997",
   "metadata": {},
   "source": [
    "<a id=\"nnLSTM\"></a>\n",
    "# Using and optimzing the PyTorch LSTM, nn.LSTM()\n",
    "\n",
    "Now that we know how to create an LSTM unit by hand, train it, and then use it to make good predictions, let's learn how to take advantage of PyTorch's `nn.LSTM()` function. For the most part, using `nn.LSTM()` allows us to simplify the `__init__()` function and the `forward()` function. The other big difference is that this time, we're not going to try and recreate the parameter values we used in the **StatQuest** on **Long Short-Term Memory**, and that means we can set the learning rate for the Adam to **0.1**. This will speed up training a lot. Everything else stays the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0781c6a1-035f-4149-b509-bf808855fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Instead of coding an LSTM by hand, let's see what we can do with PyTorch's nn.LSTM()\n",
    "class LightningLSTM(L.LightningModule):\n",
    "\n",
    "    def __init__(self): # __init__() is the class constructor function, and we use it to initialize the Weights and Biases.\n",
    "        \n",
    "        super().__init__() # initialize an instance of the parent class, LightningModule.\n",
    "\n",
    "        seed_everything(seed=42)\n",
    "        \n",
    "        ## input_size = number of features (or variables) in the data. In our example\n",
    "        ##              we only have a single feature (value)\n",
    "        ## hidden_size = this determines the dimension of the output\n",
    "        ##               in other words, if we set hidden_size=1, then we have 1 output node\n",
    "        ##               if we set hiddeen_size=50, then we hve 50 output nodes (that can then be 50 input\n",
    "        ##               nodes to a subsequent fully connected neural network.\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=1) \n",
    "         \n",
    "\n",
    "    def forward(self, input):\n",
    "        ## transpose the input vector\n",
    "        input_trans = input.view(len(input), 1)\n",
    "        \n",
    "        lstm_out, temp = self.lstm(input_trans)\n",
    "        \n",
    "        ## lstm_out has the short-term memories for all inputs. We make our prediction with the last one\n",
    "        prediction = lstm_out[-1] \n",
    "        return prediction\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self): # this configures the optimizer we want to use for backpropagation.\n",
    "        return Adam(self.parameters(), lr=0.1) ## we'll just go ahead and set the learning rate to 0.1\n",
    "\n",
    "    \n",
    "    def training_step(self, batch, batch_idx): # take a step during gradient descent.\n",
    "        input_i, label_i = batch # collect input\n",
    "        output_i = self.forward(input_i[0]) # run input through the neural network\n",
    "        loss = (output_i - label_i)**2 ## loss = squared residual\n",
    "        \n",
    "        ###################\n",
    "        ##\n",
    "        ## Logging the loss and the predicted values so we can evaluate the training\n",
    "        ##\n",
    "        ###################\n",
    "        self.log(\"train_loss\", loss)\n",
    "        \n",
    "        if (label_i == 0):\n",
    "            self.log(\"out_0\", output_i)\n",
    "        else:\n",
    "            self.log(\"out_1\", output_i)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899c78-1e93-45f3-96a5-c17654f507cc",
   "metadata": {},
   "source": [
    "Now let's create the model and print out the initial Weights and Biases and predictinos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53fbb364-34a2-4896-a788-bbd4f8621ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siyang\\Documents\\GitHub\\data-driven-fpl\\.venv\\Lib\\site-packages\\pytorch_lightning\\utilities\\seed.py:48: `pytorch_lightning.utilities.seed.seed_everything` has been deprecated in v1.8.0 and will be removed in v1.10.0. Please use `lightning_lite.utilities.seed.seed_everything` instead.\n",
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[ 0.7645],\n",
      "        [ 0.8300],\n",
      "        [-0.2343],\n",
      "        [ 0.9186]])\n",
      "lstm.weight_hh_l0 tensor([[-0.2191],\n",
      "        [ 0.2018],\n",
      "        [-0.4869],\n",
      "        [ 0.5873]])\n",
      "lstm.bias_ih_l0 tensor([ 0.8815, -0.7336,  0.8692,  0.1872])\n",
      "lstm.bias_hh_l0 tensor([ 0.7388,  0.1354,  0.4822, -0.1412])\n",
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([0.6675])\n",
      "Company B: Observed = 1, Predicted = tensor([0.6665])\n"
     ]
    }
   ],
   "source": [
    "model = LightningLSTM() # First, make model from the class\n",
    "\n",
    "## print out the name and value for each parameter\n",
    "print(\"Before optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)\n",
    "    \n",
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a1cf65-09ca-4e94-b629-e606c6030953",
   "metadata": {},
   "source": [
    "As expected, the predictions are bad, so we will train the model. However, because we've increased the learning rate to **0.1**, we only need to train for **300** epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be1c3792-9b36-4dd0-aeb2-dfbf96dc931d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "c:\\Users\\siyang\\Documents\\GitHub\\data-driven-fpl\\.venv\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 222.29it/s, v_num=4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=300` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 299: 100%|██████████| 2/2 [00:00<00:00, 133.33it/s, v_num=4]\n",
      "After optimization, the parameters are...\n",
      "lstm.weight_ih_l0 tensor([[3.5364],\n",
      "        [1.3869],\n",
      "        [1.5390],\n",
      "        [1.2488]])\n",
      "lstm.weight_hh_l0 tensor([[5.2070],\n",
      "        [2.9577],\n",
      "        [3.2652],\n",
      "        [2.0678]])\n",
      "lstm.bias_ih_l0 tensor([-0.9143,  0.3724, -0.1815,  0.6376])\n",
      "lstm.bias_hh_l0 tensor([-1.0570,  1.2414, -0.5685,  0.3092])\n"
     ]
    }
   ],
   "source": [
    "## NOTE: Because we have set Adam's learning rate to 0.1, we will train much, much faster.\n",
    "## Before, with the hand made LSTM and the default learning rate, 0.001, it took about 5000 epochs to fully train\n",
    "## the model. Now, with the learning rate set to 0.1, we only need 300 epochs. Now, because we are doing so few epochs,\n",
    "## we have to tell the trainer add stuff to the log files every 2 steps (or epoch, since we have to rows of training data)\n",
    "## because the default, updating the log files every 50 steps, will result in a terrible looking graphs. So\n",
    "trainer = L.Trainer(max_epochs=300, log_every_n_steps=2)\n",
    "\n",
    "trainer.fit(model, train_dataloaders=dataloader)\n",
    "\n",
    "print(\"After optimization, the parameters are...\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dfd472-5e19-410f-adbe-06ff89ac2efd",
   "metadata": {},
   "source": [
    "Now that training is done, let's print out the new predictions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff9fa0f2-a6a1-4806-a3e7-0d670d7f30b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Now let's compare the observed and predicted values...\n",
      "Company A: Observed = 0, Predicted = tensor([6.8491e-05])\n",
      "Company B: Observed = 1, Predicted = tensor([0.9809])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nNow let's compare the observed and predicted values...\")\n",
    "print(\"Company A: Observed = 0, Predicted =\", model(torch.tensor([0., 0.5, 0.25, 1.])).detach())\n",
    "print(\"Company B: Observed = 1, Predicted =\", model(torch.tensor([1., 0.5, 0.25, 1.])).detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41c31b-a69f-49bb-8faf-77df15a9e4f3",
   "metadata": {},
   "source": [
    "...and, as we can see, after just **300** epochs, the LSTM is making great predictions. The prediction for **Company A** is close to the observed value **0** and the prediction for **Company B** is close to the observed value **1**.\n",
    "\n",
    "Lastly, let's refresh the **TensorBoard** page to see the latest graphs. **NOTE:** To make it easier to see what we just did, deselect `version_0`, `version_1` and `version_2` and make sure `version_3` is checked on the left-hand side of the page, under where it says `Runs`. See below. This allows us to just look at the log files from the most rescent training, which only went for **300** epochs.\n",
    "\n",
    "<img src=\"./images/selecting_run_version_3.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n",
    "<img src=\"./images/train_loss_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_0_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\"><img src=\"./images/out_1_nn.lstm_300_epochs.png\" alt=\"Loss\" style=\"width: 300px;\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f184fa-befe-4c9e-a7f5-e718ab84ea6d",
   "metadata": {},
   "source": [
    "In all three graphs, the loss (`train_loss`) and the predictions for **Company A** (`out_0`) and **Company B** (`out_1`) started to taper off after **500** steps, or just **250** epochs, suggesting that adding more epochs may not improve the predictions much, so we're done!\n",
    "\n",
    "# TRIPLE BAM!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
